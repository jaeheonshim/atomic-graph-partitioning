---------------------------------------
Begin Slurm Prolog: Feb-26-2025 02:00:11
Job ID:    2930951
User ID:   jshim87
Account:   gts-vfung3
Job name:  MattersimRunner
Partition: gpu-h200
QOS:       inferno
---------------------------------------
/storage/home/hcoda1/7/jshim87/.conda/envs/graph_partitioning/lib/python3.11/site-packages/orb_models/forcefield/pretrained.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(local_path, map_location="cpu")
GPU tensorfloat matmuls precision set to 'high'. This can achieve up to 2x speedup on Nvidia A100 and H100 devices.
Traceback (most recent call last):
  File "/storage/home/hcoda1/7/jshim87/atomic-graph-partitioning/error_test.py", line 27, in <module>
    orb_partition_inference = AtomicPartitionInference(OrbModelAdapter(device=device, num_message_passing=3))
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/7/jshim87/atomic-graph-partitioning/wrapper/implementations/orb.py", line 63, in __init__
    self.orbff = load_model_for_inference(model, 'https://orbitalmaterials-public-models.s3.us-west-1.amazonaws.com/forcefields/orb-v2-20241011.ckpt', self.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/storage/home/hcoda1/7/jshim87/.conda/envs/graph_partitioning/lib/python3.11/site-packages/orb_models/forcefield/pretrained.py", line 72, in load_model_for_inference
    model.load_state_dict(state_dict, strict=True)
  File "/storage/home/hcoda1/7/jshim87/.conda/envs/graph_partitioning/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for GraphRegressor:
	Unexpected key(s) in state_dict: "model.gnn_stacks.4._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.4._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.4._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.4._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.4._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.4._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.4._node_mlp.layer_norm.weight", "model.gnn_stacks.4._node_mlp.layer_norm.bias", "model.gnn_stacks.4._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.4._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.4._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.4._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.4._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.4._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.4._edge_mlp.layer_norm.weight", "model.gnn_stacks.4._edge_mlp.layer_norm.bias", "model.gnn_stacks.4._receive_attn.weight", "model.gnn_stacks.4._receive_attn.bias", "model.gnn_stacks.4._send_attn.weight", "model.gnn_stacks.4._send_attn.bias", "model.gnn_stacks.5._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.5._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.5._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.5._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.5._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.5._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.5._node_mlp.layer_norm.weight", "model.gnn_stacks.5._node_mlp.layer_norm.bias", "model.gnn_stacks.5._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.5._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.5._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.5._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.5._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.5._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.5._edge_mlp.layer_norm.weight", "model.gnn_stacks.5._edge_mlp.layer_norm.bias", "model.gnn_stacks.5._receive_attn.weight", "model.gnn_stacks.5._receive_attn.bias", "model.gnn_stacks.5._send_attn.weight", "model.gnn_stacks.5._send_attn.bias", "model.gnn_stacks.6._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.6._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.6._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.6._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.6._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.6._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.6._node_mlp.layer_norm.weight", "model.gnn_stacks.6._node_mlp.layer_norm.bias", "model.gnn_stacks.6._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.6._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.6._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.6._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.6._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.6._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.6._edge_mlp.layer_norm.weight", "model.gnn_stacks.6._edge_mlp.layer_norm.bias", "model.gnn_stacks.6._receive_attn.weight", "model.gnn_stacks.6._receive_attn.bias", "model.gnn_stacks.6._send_attn.weight", "model.gnn_stacks.6._send_attn.bias", "model.gnn_stacks.7._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.7._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.7._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.7._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.7._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.7._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.7._node_mlp.layer_norm.weight", "model.gnn_stacks.7._node_mlp.layer_norm.bias", "model.gnn_stacks.7._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.7._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.7._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.7._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.7._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.7._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.7._edge_mlp.layer_norm.weight", "model.gnn_stacks.7._edge_mlp.layer_norm.bias", "model.gnn_stacks.7._receive_attn.weight", "model.gnn_stacks.7._receive_attn.bias", "model.gnn_stacks.7._send_attn.weight", "model.gnn_stacks.7._send_attn.bias", "model.gnn_stacks.8._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.8._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.8._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.8._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.8._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.8._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.8._node_mlp.layer_norm.weight", "model.gnn_stacks.8._node_mlp.layer_norm.bias", "model.gnn_stacks.8._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.8._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.8._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.8._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.8._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.8._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.8._edge_mlp.layer_norm.weight", "model.gnn_stacks.8._edge_mlp.layer_norm.bias", "model.gnn_stacks.8._receive_attn.weight", "model.gnn_stacks.8._receive_attn.bias", "model.gnn_stacks.8._send_attn.weight", "model.gnn_stacks.8._send_attn.bias", "model.gnn_stacks.9._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.9._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.9._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.9._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.9._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.9._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.9._node_mlp.layer_norm.weight", "model.gnn_stacks.9._node_mlp.layer_norm.bias", "model.gnn_stacks.9._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.9._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.9._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.9._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.9._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.9._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.9._edge_mlp.layer_norm.weight", "model.gnn_stacks.9._edge_mlp.layer_norm.bias", "model.gnn_stacks.9._receive_attn.weight", "model.gnn_stacks.9._receive_attn.bias", "model.gnn_stacks.9._send_attn.weight", "model.gnn_stacks.9._send_attn.bias", "model.gnn_stacks.10._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.10._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.10._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.10._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.10._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.10._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.10._node_mlp.layer_norm.weight", "model.gnn_stacks.10._node_mlp.layer_norm.bias", "model.gnn_stacks.10._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.10._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.10._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.10._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.10._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.10._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.10._edge_mlp.layer_norm.weight", "model.gnn_stacks.10._edge_mlp.layer_norm.bias", "model.gnn_stacks.10._receive_attn.weight", "model.gnn_stacks.10._receive_attn.bias", "model.gnn_stacks.10._send_attn.weight", "model.gnn_stacks.10._send_attn.bias", "model.gnn_stacks.11._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.11._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.11._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.11._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.11._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.11._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.11._node_mlp.layer_norm.weight", "model.gnn_stacks.11._node_mlp.layer_norm.bias", "model.gnn_stacks.11._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.11._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.11._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.11._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.11._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.11._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.11._edge_mlp.layer_norm.weight", "model.gnn_stacks.11._edge_mlp.layer_norm.bias", "model.gnn_stacks.11._receive_attn.weight", "model.gnn_stacks.11._receive_attn.bias", "model.gnn_stacks.11._send_attn.weight", "model.gnn_stacks.11._send_attn.bias", "model.gnn_stacks.12._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.12._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.12._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.12._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.12._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.12._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.12._node_mlp.layer_norm.weight", "model.gnn_stacks.12._node_mlp.layer_norm.bias", "model.gnn_stacks.12._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.12._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.12._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.12._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.12._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.12._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.12._edge_mlp.layer_norm.weight", "model.gnn_stacks.12._edge_mlp.layer_norm.bias", "model.gnn_stacks.12._receive_attn.weight", "model.gnn_stacks.12._receive_attn.bias", "model.gnn_stacks.12._send_attn.weight", "model.gnn_stacks.12._send_attn.bias", "model.gnn_stacks.13._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.13._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.13._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.13._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.13._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.13._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.13._node_mlp.layer_norm.weight", "model.gnn_stacks.13._node_mlp.layer_norm.bias", "model.gnn_stacks.13._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.13._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.13._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.13._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.13._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.13._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.13._edge_mlp.layer_norm.weight", "model.gnn_stacks.13._edge_mlp.layer_norm.bias", "model.gnn_stacks.13._receive_attn.weight", "model.gnn_stacks.13._receive_attn.bias", "model.gnn_stacks.13._send_attn.weight", "model.gnn_stacks.13._send_attn.bias", "model.gnn_stacks.14._node_mlp.mlp.NN-0.weight", "model.gnn_stacks.14._node_mlp.mlp.NN-0.bias", "model.gnn_stacks.14._node_mlp.mlp.NN-1.weight", "model.gnn_stacks.14._node_mlp.mlp.NN-1.bias", "model.gnn_stacks.14._node_mlp.mlp.NN-2.weight", "model.gnn_stacks.14._node_mlp.mlp.NN-2.bias", "model.gnn_stacks.14._node_mlp.layer_norm.weight", "model.gnn_stacks.14._node_mlp.layer_norm.bias", "model.gnn_stacks.14._edge_mlp.mlp.NN-0.weight", "model.gnn_stacks.14._edge_mlp.mlp.NN-0.bias", "model.gnn_stacks.14._edge_mlp.mlp.NN-1.weight", "model.gnn_stacks.14._edge_mlp.mlp.NN-1.bias", "model.gnn_stacks.14._edge_mlp.mlp.NN-2.weight", "model.gnn_stacks.14._edge_mlp.mlp.NN-2.bias", "model.gnn_stacks.14._edge_mlp.layer_norm.weight", "model.gnn_stacks.14._edge_mlp.layer_norm.bias", "model.gnn_stacks.14._receive_attn.weight", "model.gnn_stacks.14._receive_attn.bias", "model.gnn_stacks.14._send_attn.weight", "model.gnn_stacks.14._send_attn.bias". 
---------------------------------------
Begin Slurm Epilog: Feb-26-2025 02:01:52
Job ID:        2930951
User ID:       jshim87
Account:       gts-vfung3
Job name:      MattersimRunner
Resources:     cpu=1,gres/gpu:h200=1,mem=4G,node=1
Rsrc Used:     cput=00:01:41,vmem=0,walltime=00:01:41,mem=965348K,energy_used=0
Partition:     gpu-h200
QOS:           inferno
Nodes:         atl1-1-01-007-7-0
---------------------------------------
