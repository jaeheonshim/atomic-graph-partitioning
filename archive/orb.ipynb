{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Subgraph Partitioning (orb-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from partitioner import part_graph_extended\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a sample atomic dataset and converting it into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of atoms 192\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from orb_models.forcefield.atomic_system import ase_atoms_to_atom_graphs\n",
    "from ase.build import make_supercell\n",
    " \n",
    "atoms = read(\"datasets/H2O.xyz\")\n",
    "atoms = make_supercell(atoms, [[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# Instead of using neighborlist, I use the ase_atoms_to_atom_graphs provided by orb. Hopefully this will provide better results\n",
    "atom_graph = ase_atoms_to_atom_graphs(atoms) # Keep this to compare results later\n",
    "\n",
    "senders = atom_graph.senders\n",
    "receivers = atom_graph.receivers\n",
    "edge_feats = atom_graph.edge_features\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(atoms)))\n",
    "\n",
    "for i, u in enumerate(senders):\n",
    "    G.add_edge(u.item(), receivers[i].item(), weight=edge_feats['r'])\n",
    "\n",
    "print(\"Number of atoms\", len(atoms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the computational graph into the number of desired partitions with the specified neighborhood distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 20 partitions\n",
      "Average partition size: 192.0\n"
     ]
    }
   ],
   "source": [
    "desired_partitions = 20\n",
    "neighborhood_distance = 4\n",
    "partitions, extended_partitions = part_graph_extended(G, desired_partitions, neighborhood_distance)\n",
    "\n",
    "num_partitions = len(partitions)\n",
    "\n",
    "print(f\"Created {num_partitions} partitions\")\n",
    "print(f\"Average partition size: {sum(len(x) for x in extended_partitions) / num_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ASE atoms object for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "partitioned_atoms = []\n",
    "indices_map = [] # Table mapping each atom in each partition back to its index in the original atoms object\n",
    "\n",
    "for part in extended_partitions:\n",
    "    current_partition = []\n",
    "    current_indices_map = []\n",
    "    for atom_index in part:\n",
    "        current_partition.append(atoms[atom_index])\n",
    "        current_indices_map.append(atoms[atom_index].index)\n",
    "\n",
    "    partitioned_atoms.append(Atoms(current_partition, cell=atoms.cell, pbc=atoms.pbc)) # It's important to pass atoms.cell and atoms.pbc here\n",
    "    indices_map.append(current_indices_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Atoms(symbols='H128O64', pbc=True, cell=[8.65320864, 15.05202152, 14.13541336])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_atoms = []\n",
    "for atom_index in range(len(atoms)):\n",
    "    reconstructed_atoms.append(atoms[atom_index])\n",
    "reconstructed_atoms = Atoms(reconstructed_atoms, cell=atoms.cell, pbc=atoms.pbc)\n",
    "\n",
    "reconstructed_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield import atomic_system, pretrained\n",
    "from orb_models.forcefield import segment_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU tensorfloat matmuls precision set to 'high'. This can achieve up to 2x speedup on Nvidia A100 and H100 devices.\n"
     ]
    }
   ],
   "source": [
    "from orb_models.forcefield.pretrained import get_base, load_model_for_inference\n",
    "from orb_models.forcefield.graph_regressor import GraphRegressor, EnergyHead, NodeHead, GraphHead\n",
    "\n",
    "device = \"cuda\"  # or device=\"cuda\"\n",
    "\n",
    "base = get_base()\n",
    "\n",
    "model = GraphRegressor(\n",
    "    graph_head=EnergyHead(\n",
    "        latent_dim=256,\n",
    "        num_mlp_layers=1,\n",
    "        mlp_hidden_dim=256,\n",
    "        target=\"energy\",\n",
    "        node_aggregation=\"mean\",\n",
    "        reference_energy_name=\"vasp-shifted\",\n",
    "        train_reference=True,\n",
    "        predict_atom_avg=True,\n",
    "    ),\n",
    "    node_head=NodeHead(\n",
    "        latent_dim=256,\n",
    "        num_mlp_layers=1,\n",
    "        mlp_hidden_dim=256,\n",
    "        target=\"forces\",\n",
    "        remove_mean=True,\n",
    "    ),\n",
    "    stress_head=GraphHead(\n",
    "        latent_dim=256,\n",
    "        num_mlp_layers=1,\n",
    "        mlp_hidden_dim=256,\n",
    "        target=\"stress\",\n",
    "        compute_stress=True,\n",
    "    ),\n",
    "    model=base,\n",
    ")\n",
    "\n",
    "orbff = load_model_for_inference(model, 'https://orbitalmaterials-public-models.s3.us-west-1.amazonaws.com/forcefields/orb-v2-20241011.ckpt', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 45.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_features = torch.zeros((len(atoms), 256), dtype=torch.float32, device=device)\n",
    "\n",
    "for i, part in tqdm(enumerate(partitioned_atoms), total=num_partitions):\n",
    "    input_graph = atomic_system.ase_atoms_to_atom_graphs(part)\n",
    "\n",
    "    batch = orbff.model(input_graph)\n",
    "\n",
    "    feat = batch.node_features[\"feat\"]\n",
    "\n",
    "    for j, node in enumerate(part):\n",
    "        original_index = indices_map[i][j]\n",
    "        if original_index in partitions[i]: # If the node is a root node of the partition\n",
    "            aggregated_features[original_index] = feat[j]\n",
    "\n",
    "aggregated_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the original, unpartitioned graph to obtain a benchmark for our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-952.4021], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = orbff.predict(atom_graph)\n",
    "benchmark_energy = result[\"graph_pred\"]\n",
    "benchmark_forces = result[\"node_pred\"]\n",
    "benchmark_stress = result[\"stress_pred\"]\n",
    "benchmark_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield.graph_regressor import ScalarNormalizer, LinearReferenceEnergy\n",
    "from orb_models.forcefield.reference_energies import REFERENCE_ENERGIES\n",
    "\n",
    "ref = REFERENCE_ENERGIES[\"vasp-shifted\"]\n",
    "reference = LinearReferenceEnergy(\n",
    "    weight_init=ref.coefficients, trainable=True\n",
    ")\n",
    "\n",
    "n_node = torch.tensor([aggregated_features.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43msegment_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregated_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m energy \u001b[38;5;241m=\u001b[39m orbff\u001b[38;5;241m.\u001b[39mgraph_head\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m      8\u001b[0m energy \u001b[38;5;241m=\u001b[39m orbff\u001b[38;5;241m.\u001b[39mgraph_head\u001b[38;5;241m.\u001b[39mnormalizer\u001b[38;5;241m.\u001b[39minverse(energy)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/partitioning/lib/python3.11/site-packages/orb_models/forcefield/segment_ops.py:45\u001b[0m, in \u001b[0;36maggregate_nodes\u001b[0;34m(tensor, n_node, reduction, deterministic)\u001b[0m\n\u001b[1;32m     43\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUBLAS_WORKSPACE_CONFIG\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:4096:8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m     torch\u001b[38;5;241m.\u001b[39muse_deterministic_algorithms(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_sum(tensor, segments, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "energy = orbff.graph_head.mlp(input)\n",
    "energy = orbff.graph_head.normalizer.inverse(energy).squeeze(-1)\n",
    "energy = energy * n_node\n",
    "energy = energy + reference(atom_graph.atomic_numbers, atom_graph.n_node)\n",
    "energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute error: 0.0\n",
      "Percent error: 0.0%\n",
      "Maximum error: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute error: {torch.abs(benchmark_energy - energy).item()}\")\n",
    "print(f\"Percent error: {torch.abs((benchmark_energy - energy) / benchmark_energy).item() * 100}%\")\n",
    "print(f\"Maximum error: {torch.max(benchmark_energy - energy).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0688,  0.0882,  0.2663],\n",
       "        [ 0.0824,  0.1349,  0.2663],\n",
       "        [ 0.0129, -0.0198,  0.2584],\n",
       "        ...,\n",
       "        [ 0.0539, -0.0196,  0.1435],\n",
       "        [-0.0231,  0.0474,  0.1652],\n",
       "        [ 0.0963,  0.0796,  0.1533]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forces = orbff.node_head.mlp(aggregated_features)\n",
    "system_means = segment_ops.aggregate_nodes(\n",
    "    forces, n_node, reduction=\"mean\"\n",
    ")\n",
    "node_broadcasted_means = torch.repeat_interleave(\n",
    "    system_means, n_node, dim=0\n",
    ")\n",
    "forces = forces - node_broadcasted_means\n",
    "forces = orbff.node_head.normalizer.inverse(forces)\n",
    "forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.0\n",
      "Mean absolute percent error: 0.0%\n",
      "Maximum error: 0.0\n"
     ]
    }
   ],
   "source": [
    "mae = torch.mean(torch.abs(benchmark_forces - forces))\n",
    "mape = 100 * torch.mean(torch.abs((benchmark_forces - forces) / benchmark_forces))\n",
    "max = torch.max(torch.abs(benchmark_forces - forces))\n",
    "\n",
    "print(f\"Mean absolute error: {mae.item()}\")\n",
    "print(f\"Mean absolute percent error: {mape}%\")\n",
    "print(f\"Maximum error: {max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4797e-02, -1.7188e-02, -1.2462e-02,  2.4029e-05,  3.1426e-05,\n",
       "          5.2789e-07]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\",\n",
    ")\n",
    "stress = orbff.stress_head.mlp(input)\n",
    "stress = stress.squeeze(-1)\n",
    "stress = orbff.stress_head.output_activation(stress)\n",
    "stress = orbff.stress_head.normalizer.inverse(stress)\n",
    "stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.0\n",
      "Mean absolute percent error: 0.0%\n",
      "Max error: 0.0\n"
     ]
    }
   ],
   "source": [
    "mae = torch.mean(torch.abs(benchmark_stress - stress))\n",
    "mape = 100 * torch.mean(torch.abs((benchmark_stress - stress) / benchmark_stress))\n",
    "max = torch.max(torch.abs(benchmark_stress - stress))\n",
    "print(f\"Mean absolute error: {mae.item()}\")\n",
    "print(f\"Mean absolute percent error: {mape}%\")\n",
    "print(f\"Max error: {max}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "partitioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
