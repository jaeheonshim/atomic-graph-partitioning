{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Subgraph Partitioning (orb-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from partitioner import part_graph_extended\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a sample atomic dataset and converting it into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 116.82 GiB. GPU 0 has a total capacity of 79.14 GiB of which 5.44 MiB is free. Process 1150784 has 78.57 GiB memory in use. Including non-PyTorch memory, this process has 562.00 MiB memory in use. Of the allocated memory 51.37 MiB is allocated by PyTorch, and 20.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m atoms \u001b[38;5;241m=\u001b[39m make_supercell(atoms, [[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m]])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Instead of using neighborlist, I use the ase_atoms_to_atom_graphs provided by orb. Hopefully this will provide better results\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m atom_graph \u001b[38;5;241m=\u001b[39m \u001b[43mase_atoms_to_atom_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43matoms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Keep this to compare results later\u001b[39;00m\n\u001b[1;32m     11\u001b[0m senders \u001b[38;5;241m=\u001b[39m atom_graph\u001b[38;5;241m.\u001b[39msenders\n\u001b[1;32m     12\u001b[0m receivers \u001b[38;5;241m=\u001b[39m atom_graph\u001b[38;5;241m.\u001b[39mreceivers\n",
      "File \u001b[0;32m~/.conda/envs/partitioning/lib/python3.11/site-packages/orb_models/forcefield/atomic_system.py:141\u001b[0m, in \u001b[0;36mase_atoms_to_atom_graphs\u001b[0;34m(atoms, wrap, brute_force_knn, device, system_config, system_id)\u001b[0m\n\u001b[1;32m    133\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matomic_numbers\u001b[39m\u001b[38;5;124m\"\u001b[39m: atomic_numbers\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matomic_numbers_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: atom_type_embedding\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m: positions,\n\u001b[1;32m    139\u001b[0m }\n\u001b[1;32m    140\u001b[0m system_feats \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell\u001b[39m\u001b[38;5;124m\"\u001b[39m: cell\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)}\n\u001b[0;32m--> 141\u001b[0m edge_feats, senders, receivers \u001b[38;5;241m=\u001b[39m \u001b[43m_get_edge_feats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_num_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrute_force\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrute_force_knn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m num_atoms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node_feats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    151\u001b[0m atom_graph \u001b[38;5;241m=\u001b[39m AtomGraphs(\n\u001b[1;32m    152\u001b[0m     senders\u001b[38;5;241m=\u001b[39msenders,\n\u001b[1;32m    153\u001b[0m     receivers\u001b[38;5;241m=\u001b[39mreceivers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     max_num_neighbors\u001b[38;5;241m=\u001b[39msystem_config\u001b[38;5;241m.\u001b[39mmax_num_neighbors,\n\u001b[1;32m    164\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/partitioning/lib/python3.11/site-packages/orb_models/forcefield/atomic_system.py:193\u001b[0m, in \u001b[0;36m_get_edge_feats\u001b[0;34m(positions, cell, radius, max_num_neighbours, brute_force, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get edge features.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    device: device to put the tensors on.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Construct a graph from a 3x3 supercell (as opposed to an infinite supercell).\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# This could be innaccurate for thin unit cells, but we have yet to encounter a\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# major issue and this approach is faster.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m (\n\u001b[1;32m    191\u001b[0m     edge_index,\n\u001b[1;32m    192\u001b[0m     edge_vectors,\n\u001b[0;32m--> 193\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mfeaturization_utilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_pbc_radius_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperiodic_boundaries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_number_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_neighbours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrute_force\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrute_force\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m edge_feats \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m\"\u001b[39m: edge_vectors\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m: edge_vectors\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    204\u001b[0m }\n\u001b[1;32m    205\u001b[0m senders, receivers \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m], edge_index[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/partitioning/lib/python3.11/site-packages/orb_models/forcefield/featurization_utilities.py:291\u001b[0m, in \u001b[0;36mcompute_pbc_radius_graph\u001b[0;34m(positions, periodic_boundaries, radius, max_number_neighbors, brute_force, library, n_workers, device)\u001b[0m\n\u001b[1;32m    287\u001b[0m num_positions \u001b[38;5;241m=\u001b[39m positions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m brute_force:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# Brute force\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     distance_values, nearest_img_neighbors \u001b[38;5;241m=\u001b[39m \u001b[43mbrute_force_knn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupercell_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_number_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msupercell_positions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# remove distances greater than radius, and exclude self\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     within_radius \u001b[38;5;241m=\u001b[39m distance_values[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m<\u001b[39m (radius \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/partitioning/lib/python3.11/site-packages/orb_models/forcefield/featurization_utilities.py:211\u001b[0m, in \u001b[0;36mbrute_force_knn\u001b[0;34m(img_positions, positions, k)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbrute_force_knn\u001b[39m(\n\u001b[1;32m    199\u001b[0m     img_positions: torch\u001b[38;5;241m.\u001b[39mTensor, positions: torch\u001b[38;5;241m.\u001b[39mTensor, k: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Brute force k-nearest neighbors.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m        torch.return_types.topk: The indices of the nearest neighbors. Shape [num_atoms, k].\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_positions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtopk(dist, k, largest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/partitioning/lib/python3.11/site-packages/torch/functional.py:1336\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1334\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 116.82 GiB. GPU 0 has a total capacity of 79.14 GiB of which 5.44 MiB is free. Process 1150784 has 78.57 GiB memory in use. Including non-PyTorch memory, this process has 562.00 MiB memory in use. Of the allocated memory 51.37 MiB is allocated by PyTorch, and 20.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from orb_models.forcefield.atomic_system import ase_atoms_to_atom_graphs\n",
    "from ase.build import make_supercell\n",
    " \n",
    "atoms = read(\"datasets/test.xyz\")\n",
    "atoms = make_supercell(atoms, [[5, 0, 0], [0, 4, 0], [0, 0, 4]])\n",
    "\n",
    "# Instead of using neighborlist, I use the ase_atoms_to_atom_graphs provided by orb. Hopefully this will provide better results\n",
    "atom_graph = ase_atoms_to_atom_graphs(atoms) # Keep this to compare results later\n",
    "\n",
    "senders = atom_graph.senders\n",
    "receivers = atom_graph.receivers\n",
    "edge_feats = atom_graph.edge_features\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(atoms)))\n",
    "\n",
    "for i, u in enumerate(senders):\n",
    "    G.add_edge(u.item(), receivers[i].item(), weight=edge_feats['r'])\n",
    "\n",
    "print(\"Number of atoms\", len(atoms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the computational graph into the number of desired partitions with the specified neighborhood distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_partitions = 20\n",
    "neighborhood_distance = 4\n",
    "partitions, extended_partitions = part_graph_extended(G, desired_partitions, neighborhood_distance)\n",
    "\n",
    "num_partitions = len(partitions)\n",
    "\n",
    "print(f\"Created {num_partitions} partitions\")\n",
    "print(f\"Average partition size: {sum(len(x) for x in extended_partitions) / num_partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ASE atoms object for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "partitioned_atoms = []\n",
    "indices_map = [] # Table mapping each atom in each partition back to its index in the original atoms object\n",
    "\n",
    "for part in extended_partitions:\n",
    "    current_partition = []\n",
    "    current_indices_map = []\n",
    "    for atom_index in part:\n",
    "        current_partition.append(atoms[atom_index])\n",
    "        current_indices_map.append(atoms[atom_index].index)\n",
    "\n",
    "    partitioned_atoms.append(Atoms(current_partition, cell=atoms.cell, pbc=atoms.pbc)) # It's important to pass atoms.cell and atoms.pbc here\n",
    "    indices_map.append(current_indices_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_atoms = []\n",
    "for atom_index in range(len(atoms)):\n",
    "    reconstructed_atoms.append(atoms[atom_index])\n",
    "reconstructed_atoms = Atoms(reconstructed_atoms, cell=atoms.cell, pbc=atoms.pbc)\n",
    "\n",
    "reconstructed_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield import atomic_system, pretrained\n",
    "from orb_models.forcefield import segment_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield.pretrained import get_base, load_model_for_inference\n",
    "from orb_models.forcefield.graph_regressor import GraphRegressor, EnergyHead, NodeHead, GraphHead\n",
    "\n",
    "device = \"cpu\"  # or device=\"cuda\"\n",
    "\n",
    "base = get_base(num_message_passing_steps=4)\n",
    "\n",
    "model = GraphRegressor(\n",
    "    graph_head=EnergyHead(\n",
    "        latent_dim=256,\n",
    "        num_mlp_layers=1,\n",
    "        mlp_hidden_dim=256,\n",
    "        target=\"energy\",\n",
    "        node_aggregation=\"mean\",\n",
    "        reference_energy_name=\"vasp-shifted\",\n",
    "        train_reference=True,\n",
    "        predict_atom_avg=True,\n",
    "    ),\n",
    "    node_head=NodeHead(\n",
    "        latent_dim=256,\n",
    "        num_mlp_layers=1,\n",
    "        mlp_hidden_dim=256,\n",
    "        target=\"forces\",\n",
    "        remove_mean=True,\n",
    "    ),\n",
    "    stress_head=GraphHead(\n",
    "        latent_dim=256,\n",
    "        num_mlp_layers=1,\n",
    "        mlp_hidden_dim=256,\n",
    "        target=\"stress\",\n",
    "        compute_stress=True,\n",
    "    ),\n",
    "    model=base,\n",
    ")\n",
    "\n",
    "orbff = load_model_for_inference(model, 'https://orbitalmaterials-public-models.s3.us-west-1.amazonaws.com/forcefields/orb-v2-20241011.ckpt', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_features = torch.zeros((len(atoms), 256), dtype=torch.float32, device=device)\n",
    "\n",
    "for i, part in tqdm(enumerate(partitioned_atoms), total=num_partitions):\n",
    "    input_graph = atomic_system.ase_atoms_to_atom_graphs(part)\n",
    "\n",
    "    batch = orbff.model(input_graph)\n",
    "\n",
    "    feat = batch.node_features[\"feat\"]\n",
    "\n",
    "    for j, node in enumerate(part):\n",
    "        original_index = indices_map[i][j]\n",
    "        if original_index in partitions[i]: # If the node is a root node of the partition\n",
    "            aggregated_features[original_index] = feat[j]\n",
    "\n",
    "aggregated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the original, unpartitioned graph to obtain a benchmark for our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = orbff.predict(atom_graph)\n",
    "benchmark_energy = result[\"graph_pred\"]\n",
    "benchmark_forces = result[\"node_pred\"]\n",
    "benchmark_stress = result[\"stress_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield.graph_regressor import ScalarNormalizer, LinearReferenceEnergy\n",
    "from orb_models.forcefield.reference_energies import REFERENCE_ENERGIES\n",
    "\n",
    "ref = REFERENCE_ENERGIES[\"vasp-shifted\"]\n",
    "reference = LinearReferenceEnergy(\n",
    "    weight_init=ref.coefficients, trainable=True\n",
    ")\n",
    "\n",
    "n_node = torch.tensor([aggregated_features.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "energy = orbff.graph_head.mlp(input)\n",
    "energy = orbff.graph_head.normalizer.inverse(energy).squeeze(-1)\n",
    "energy = energy * n_node\n",
    "energy = energy + reference(atom_graph.atomic_numbers, atom_graph.n_node)\n",
    "energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Absolute error: {torch.abs(benchmark_energy - energy).item()}\")\n",
    "print(f\"Percent error: {torch.abs((benchmark_energy - energy) / benchmark_energy).item() * 100}%\")\n",
    "print(f\"Maximum error: {torch.max(benchmark_energy - energy).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forces = orbff.node_head.mlp(aggregated_features)\n",
    "system_means = segment_ops.aggregate_nodes(\n",
    "    forces, n_node, reduction=\"mean\"\n",
    ")\n",
    "node_broadcasted_means = torch.repeat_interleave(\n",
    "    system_means, n_node, dim=0\n",
    ")\n",
    "forces = forces - node_broadcasted_means\n",
    "forces = orbff.node_head.normalizer.inverse(forces)\n",
    "forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = torch.mean(torch.abs(benchmark_forces - forces))\n",
    "mape = 100 * torch.mean(torch.abs((benchmark_forces - forces) / benchmark_forces))\n",
    "max = torch.max(torch.abs(benchmark_forces - forces))\n",
    "\n",
    "print(f\"Mean absolute error: {mae.item()}\")\n",
    "print(f\"Mean absolute percent error: {mape}%\")\n",
    "print(f\"Maximum error: {max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\",\n",
    ")\n",
    "stress = orbff.stress_head.mlp(input)\n",
    "stress = stress.squeeze(-1)\n",
    "stress = orbff.stress_head.output_activation(stress)\n",
    "stress = orbff.stress_head.normalizer.inverse(stress)\n",
    "stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = torch.mean(torch.abs(benchmark_stress - stress))\n",
    "mape = 100 * torch.mean(torch.abs((benchmark_stress - stress) / benchmark_stress))\n",
    "max = torch.max(torch.abs(benchmark_stress - stress))\n",
    "print(f\"Mean absolute error: {mae.item()}\")\n",
    "print(f\"Mean absolute percent error: {mape}%\")\n",
    "print(f\"Max error: {max}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (partitioning)",
   "language": "python",
   "name": "partitioning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
