{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Subgraph Partitioning Mattertune\n",
    "Now I will use the partitioning algorithm for inference using Mattertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import ase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import metis\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def part_graph_extended(G, desired_partitions, distance=None):\n",
    "    def descendants_at_distance_multisource(G, sources, distance=None):\n",
    "        if sources in G:\n",
    "            sources = [sources]\n",
    "\n",
    "        queue = deque(sources)\n",
    "        depths = deque([0 for _ in queue])\n",
    "        visited = set(sources)\n",
    "\n",
    "        for source in queue:\n",
    "            if source not in G:\n",
    "                raise nx.NetworkXError(f\"The node {source} is not in the graph.\")\n",
    "\n",
    "        while queue:\n",
    "            node = queue[0]\n",
    "            depth = depths[0]\n",
    "\n",
    "            if distance is not None and depth > distance: return\n",
    "\n",
    "            yield queue[0]\n",
    "\n",
    "            queue.popleft()\n",
    "            depths.popleft()\n",
    "\n",
    "            for child in G[node]:\n",
    "                if child not in visited:\n",
    "                    visited.add(child)\n",
    "                    queue.append(child)\n",
    "                    depths.append(depth + 1)\n",
    "\n",
    "    _, parts = metis.part_graph(G, desired_partitions, objtype=\"cut\")\n",
    "    partition_map = {node: parts[i] for i, node in enumerate(G.nodes())}\n",
    "    num_partitions = desired_partitions\n",
    "\n",
    "    # Find indices of nodes in each partition\n",
    "    partitions = [set() for _ in range(desired_partitions)]\n",
    "\n",
    "    for i, node in enumerate(G.nodes()):\n",
    "        partitions[partition_map[i]].add(node)\n",
    "\n",
    "    # Find boundary nodes (vertices adjacent to vertex not in partition)\n",
    "    boundary_nodes = [set(map(lambda uv: uv[0], nx.edge_boundary(G, partitions[i]))) for i in range(num_partitions)]\n",
    "\n",
    "    # Perform BFS on boundary_nodes to find extended neighbors up to a certain distance\n",
    "    extended_neighbors = [set(descendants_at_distance_multisource(G, boundary_nodes[i], distance=distance)) for i in range(num_partitions)]\n",
    "\n",
    "    extended_partitions = [p.union(a) for p, a in zip(partitions, extended_neighbors)]\n",
    "\n",
    "    return partitions, extended_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a sample atomic dataset and converting it into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of atoms 3408\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from orb_models.forcefield.atomic_system import ase_atoms_to_atom_graphs\n",
    "from ase.build import make_supercell\n",
    " \n",
    "atoms = read(\"datasets/test.xyz\")\n",
    "atoms = make_supercell(atoms, [[2, 0, 0], [0, 2, 0], [0, 0, 2]])\n",
    "\n",
    "# Instead of using neighborlist, I use the ase_atoms_to_atom_graphs provided by orb. Hopefully this will provide better results\n",
    "atom_graph = ase_atoms_to_atom_graphs(atoms) # Keep this to compare results later\n",
    "\n",
    "senders = atom_graph.senders\n",
    "receivers = atom_graph.receivers\n",
    "edge_feats = atom_graph.edge_features\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(atoms)))\n",
    "\n",
    "for i, u in enumerate(senders):\n",
    "    G.add_edge(u.item(), receivers[i].item(), weight=edge_feats['r'])\n",
    "\n",
    "print(\"Number of atoms\", len(atoms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the computational graph into the number of desired partitions with the specified neighborhood distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_partitions = 2\n",
    "neighborhood_distance = 10\n",
    "partitions, extended_partitions = part_graph_extended(G, desired_partitions, neighborhood_distance)\n",
    "\n",
    "num_partitions = len(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ASE atoms object for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "partitioned_atoms = []\n",
    "indices_map = [] # Table mapping each atom in each partition back to its index in the original atoms object\n",
    "\n",
    "for part in extended_partitions:\n",
    "    current_partition = []\n",
    "    current_indices_map = []\n",
    "    for atom_index in part:\n",
    "        current_partition.append(atoms[atom_index])\n",
    "        current_indices_map.append(atoms[atom_index].index)\n",
    "\n",
    "    partitioned_atoms.append(Atoms(current_partition, cell=atoms.cell, pbc=atoms.pbc))\n",
    "    indices_map.append(current_indices_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Atoms(symbols='C880H2208Ga64S64Si192', pbc=True, cell=[[23.096664428710938, 13.334883689880371, 23.9624080657959], [-23.09648895263672, 13.334826469421387, 23.962270736694336], [3.831999856629409e-05, -26.669597625732422, 23.962278366088867]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_atoms = []\n",
    "for atom_index in range(len(atoms)):\n",
    "    reconstructed_atoms.append(atoms[atom_index])\n",
    "reconstructed_atoms = Atoms(reconstructed_atoms, cell=atoms.cell, pbc=atoms.pbc)\n",
    "\n",
    "reconstructed_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield import atomic_system, pretrained\n",
    "from orb_models.forcefield import segment_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/orb-partitioning/lib/python3.10/site-packages/orb_models/forcefield/pretrained.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(local_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"  # or device=\"cuda\"\n",
    "\n",
    "orbff = pretrained.orb_v2(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1421753/4047243085.py:13: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  aggregated_features[original_index] = feat[j]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3629,  0.6121, -0.2159,  ..., -0.1818,  0.5214,  0.2927],\n",
       "        [-0.1475,  0.8231,  0.1479,  ..., -0.0515,  0.2717, -0.3499],\n",
       "        [-0.5161,  0.6599, -0.2023,  ...,  0.0230,  0.3217, -0.8838],\n",
       "        ...,\n",
       "        [ 0.4505, -0.0583,  0.0022,  ...,  0.0957, -0.1776, -0.0512],\n",
       "        [ 0.7821,  0.0331,  0.2452,  ..., -0.0250, -0.0294, -0.4117],\n",
       "        [ 0.8261, -0.0955,  0.0826,  ...,  0.1450, -0.5848, -0.3101]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_features = np.zeros((len(atoms), 256), dtype=np.float32)\n",
    "\n",
    "for i, part in enumerate(partitioned_atoms):\n",
    "    input_graph = atomic_system.ase_atoms_to_atom_graphs(part)\n",
    "\n",
    "    batch = orbff.model(input_graph)\n",
    "\n",
    "    feat = batch.node_features[\"feat\"]\n",
    "\n",
    "    for j, node in enumerate(part):\n",
    "        original_index = indices_map[i][j]\n",
    "        if original_index in partitions[i]: # If the node is a root node of the partition\n",
    "            aggregated_features[original_index] = feat[j]\n",
    "\n",
    "aggregated_features = torch.from_numpy(aggregated_features)\n",
    "aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-17688.2305], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the MLP\n",
    "from orb_models.forcefield.nn_util import build_mlp\n",
    "from orb_models.forcefield.graph_regressor import ScalarNormalizer, LinearReferenceEnergy\n",
    "from orb_models.forcefield.reference_energies import REFERENCE_ENERGIES\n",
    "\n",
    "normalizer = ScalarNormalizer()\n",
    "\n",
    "ref = REFERENCE_ENERGIES[\"vasp-shifted\"]\n",
    "reference = LinearReferenceEnergy(\n",
    "    weight_init=ref.coefficients, trainable=True\n",
    ")\n",
    "\n",
    "n_node = torch.tensor([aggregated_features.shape[0]])\n",
    "\n",
    "mlp = build_mlp(\n",
    "    input_size=256,\n",
    "    hidden_layer_sizes=[256] * 1,\n",
    "    output_size=1,\n",
    ")\n",
    "\n",
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "pred = orbff.graph_head.mlp(input)\n",
    "pred = orbff.graph_head.normalizer.inverse(pred).squeeze(-1)\n",
    "pred = pred * n_node\n",
    "pred = pred + reference(atom_graph.atomic_numbers, atom_graph.n_node)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graph_pred': tensor([-17688.2305]), 'stress_pred': tensor([[ 1.1084e-02,  1.1161e-02,  1.2405e-02,  8.0175e-06, -1.8519e-06,\n",
      "         -2.5269e-06]]), 'node_pred': tensor([[ 0.0349, -0.0312,  0.0005],\n",
      "        [-0.0958, -0.0764, -0.0108],\n",
      "        [-0.0572,  0.0509,  0.0085],\n",
      "        ...,\n",
      "        [ 0.0123, -0.1079,  0.0407],\n",
      "        [-0.1089,  0.0325,  0.0415],\n",
      "        [ 0.0791,  0.0703,  0.0156]])}\n"
     ]
    }
   ],
   "source": [
    "result = orbff.predict(atom_graph)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_feats = batch.node_features[\"feat\"]\n",
    "torch.mean((aggregated_features - real_feats) / real_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forces_from_partition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43mforces_from_partition\u001b[49m \u001b[38;5;241m-\u001b[39m forces_from_original) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m mae \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mabs\u001b[39m(forces_from_partition \u001b[38;5;241m-\u001b[39m forces_from_original))\n\u001b[1;32m      3\u001b[0m mape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mabs\u001b[39m(forces_from_partition \u001b[38;5;241m-\u001b[39m forces_from_original) \u001b[38;5;241m/\u001b[39m forces_from_original)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'forces_from_partition' is not defined"
     ]
    }
   ],
   "source": [
    "mse = torch.mean((forces_from_partition - forces_from_original) ** 2)\n",
    "mae = torch.mean(abs(forces_from_partition - forces_from_original))\n",
    "mape = 100 * torch.mean(abs(forces_from_partition - forces_from_original) / forces_from_original)\n",
    "mse, mae, mape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orb-partitioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
