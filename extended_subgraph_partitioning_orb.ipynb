{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Subgraph Partitioning Mattertune\n",
    "Now I will use the partitioning algorithm for inference using Mattertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import metis\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def part_graph_extended(G, desired_partitions, distance=None):\n",
    "    def descendants_at_distance_multisource(G, sources, distance=None):\n",
    "        if sources in G:\n",
    "            sources = [sources]\n",
    "\n",
    "        queue = deque(sources)\n",
    "        depths = deque([0 for _ in queue])\n",
    "        visited = set(sources)\n",
    "\n",
    "        for source in queue:\n",
    "            if source not in G:\n",
    "                raise nx.NetworkXError(f\"The node {source} is not in the graph.\")\n",
    "\n",
    "        while queue:\n",
    "            node = queue[0]\n",
    "            depth = depths[0]\n",
    "\n",
    "            if distance is not None and depth > distance: return\n",
    "\n",
    "            yield queue[0]\n",
    "\n",
    "            queue.popleft()\n",
    "            depths.popleft()\n",
    "\n",
    "            for child in G[node]:\n",
    "                if child not in visited:\n",
    "                    visited.add(child)\n",
    "                    queue.append(child)\n",
    "                    depths.append(depth + 1)\n",
    "\n",
    "    _, parts = metis.part_graph(G, desired_partitions, objtype=\"cut\")\n",
    "    partition_map = {node: parts[i] for i, node in enumerate(G.nodes())}\n",
    "    num_partitions = desired_partitions\n",
    "\n",
    "    # Find indices of nodes in each partition\n",
    "    partitions = [set() for _ in range(desired_partitions)]\n",
    "\n",
    "    for i, node in enumerate(G.nodes()):\n",
    "        partitions[partition_map[i]].add(node)\n",
    "\n",
    "    # Find boundary nodes (vertices adjacent to vertex not in partition)\n",
    "    boundary_nodes = [set(map(lambda uv: uv[0], nx.edge_boundary(G, partitions[i]))) for i in range(num_partitions)]\n",
    "\n",
    "    # Perform BFS on boundary_nodes to find extended neighbors up to a certain distance\n",
    "    extended_neighbors = [set(descendants_at_distance_multisource(G, boundary_nodes[i], distance=distance)) for i in range(num_partitions)]\n",
    "\n",
    "    extended_partitions = [p.union(a) for p, a in zip(partitions, extended_neighbors)]\n",
    "\n",
    "    return partitions, extended_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a sample atomic dataset and converting it into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of atoms 3408\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from orb_models.forcefield.atomic_system import ase_atoms_to_atom_graphs\n",
    "from ase.build import make_supercell\n",
    " \n",
    "atoms = read(\"datasets/test.xyz\")\n",
    "atoms = make_supercell(atoms, [[2, 0, 0], [0, 2, 0], [0, 0, 2]])\n",
    "\n",
    "# Instead of using neighborlist, I use the ase_atoms_to_atom_graphs provided by orb. Hopefully this will provide better results\n",
    "atom_graph = ase_atoms_to_atom_graphs(atoms) # Keep this to compare results later\n",
    "\n",
    "senders = atom_graph.senders\n",
    "receivers = atom_graph.receivers\n",
    "edge_feats = atom_graph.edge_features\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(atoms)))\n",
    "\n",
    "for i, u in enumerate(senders):\n",
    "    G.add_edge(u.item(), receivers[i].item(), weight=edge_feats['r'])\n",
    "\n",
    "print(\"Number of atoms\", len(atoms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the computational graph into the number of desired partitions with the specified neighborhood distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_partitions = 20\n",
    "neighborhood_distance = 3\n",
    "partitions, extended_partitions = part_graph_extended(G, desired_partitions, neighborhood_distance)\n",
    "\n",
    "num_partitions = len(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ASE atoms object for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "partitioned_atoms = []\n",
    "indices_map = [] # Table mapping each atom in each partition back to its index in the original atoms object\n",
    "\n",
    "for part in extended_partitions:\n",
    "    current_partition = []\n",
    "    current_indices_map = []\n",
    "    for atom_index in part:\n",
    "        current_partition.append(atoms[atom_index])\n",
    "        current_indices_map.append(atoms[atom_index].index)\n",
    "\n",
    "    partitioned_atoms.append(Atoms(current_partition, cell=atoms.cell, pbc=atoms.pbc))\n",
    "    indices_map.append(current_indices_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Atoms(symbols='C880H2208Ga64S64Si192', pbc=True, cell=[[23.096664428710938, 13.334883689880371, 23.9624080657959], [-23.09648895263672, 13.334826469421387, 23.962270736694336], [3.831999856629409e-05, -26.669597625732422, 23.962278366088867]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_atoms = []\n",
    "for atom_index in range(len(atoms)):\n",
    "    reconstructed_atoms.append(atoms[atom_index])\n",
    "reconstructed_atoms = Atoms(reconstructed_atoms, cell=atoms.cell, pbc=atoms.pbc)\n",
    "\n",
    "reconstructed_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield import atomic_system, pretrained\n",
    "from orb_models.forcefield import segment_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/orb-partitioning/lib/python3.10/site-packages/orb_models/forcefield/pretrained.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(local_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"  # or device=\"cuda\"\n",
    "\n",
    "orbff = pretrained.orb_v2(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:51<00:00,  5.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3705,  0.6076, -0.2275,  ..., -0.1806,  0.5268,  0.2838],\n",
       "        [-0.1514,  0.8388,  0.1421,  ..., -0.0490,  0.2647, -0.3448],\n",
       "        [-0.5362,  0.6659, -0.2096,  ...,  0.0278,  0.3660, -0.8769],\n",
       "        ...,\n",
       "        [ 0.4537, -0.0585,  0.0067,  ...,  0.0984, -0.1770, -0.0481],\n",
       "        [ 0.7884,  0.0315,  0.2478,  ..., -0.0237, -0.0325, -0.4072],\n",
       "        [ 0.8343, -0.0955,  0.0849,  ...,  0.1475, -0.5810, -0.3088]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_features = torch.zeros((len(atoms), 256), dtype=torch.float32, device=device)\n",
    "\n",
    "for i, part in tqdm(enumerate(partitioned_atoms), total=num_partitions):\n",
    "    input_graph = atomic_system.ase_atoms_to_atom_graphs(part)\n",
    "\n",
    "    batch = orbff.model(input_graph)\n",
    "\n",
    "    feat = batch.node_features[\"feat\"]\n",
    "\n",
    "    for j, node in enumerate(part):\n",
    "        original_index = indices_map[i][j]\n",
    "        if original_index in partitions[i]: # If the node is a root node of the partition\n",
    "            aggregated_features[original_index] = feat[j]\n",
    "\n",
    "aggregated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the prediction on the original, unpartitioned graph to obtain a benchmark for our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = orbff.predict(atom_graph)\n",
    "benchmark_energy = result[\"graph_pred\"]\n",
    "benchmark_forces = result[\"node_pred\"]\n",
    "benchmark_stress = result[\"stress_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield.graph_regressor import ScalarNormalizer, LinearReferenceEnergy\n",
    "from orb_models.forcefield.reference_energies import REFERENCE_ENERGIES\n",
    "\n",
    "ref = REFERENCE_ENERGIES[\"vasp-shifted\"]\n",
    "reference = LinearReferenceEnergy(\n",
    "    weight_init=ref.coefficients, trainable=True\n",
    ")\n",
    "\n",
    "n_node = torch.tensor([aggregated_features.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-17688.4375], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "energy = orbff.graph_head.mlp(input)\n",
    "energy = orbff.graph_head.normalizer.inverse(energy).squeeze(-1)\n",
    "energy = energy * n_node\n",
    "energy = energy + reference(atom_graph.atomic_numbers, atom_graph.n_node)\n",
    "energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute error: 0.20703125\n",
      "Percent error: 0.0011704463759087957%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute error: {torch.abs(benchmark_energy - energy).item()}\")\n",
    "print(f\"Percent error: {torch.abs((benchmark_energy - energy) / benchmark_energy).item() * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0061, -0.0097,  0.0199],\n",
       "        [-0.0710, -0.0580, -0.0067],\n",
       "        [-0.0459, -0.0113,  0.0353],\n",
       "        ...,\n",
       "        [ 0.0153, -0.1068,  0.0393],\n",
       "        [-0.1074,  0.0386,  0.0392],\n",
       "        [ 0.0803,  0.0699,  0.0097]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forces = orbff.node_head.mlp(aggregated_features)\n",
    "system_means = segment_ops.aggregate_nodes(\n",
    "    forces, n_node, reduction=\"mean\"\n",
    ")\n",
    "node_broadcasted_means = torch.repeat_interleave(\n",
    "    system_means, n_node, dim=0\n",
    ")\n",
    "forces = forces - node_broadcasted_means\n",
    "forces = orbff.node_head.normalizer.inverse(forces)\n",
    "forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.0007215619552880526\n",
      "Mean absolute percent error: 2.961912155151367%\n"
     ]
    }
   ],
   "source": [
    "mae = torch.mean(torch.abs(benchmark_forces - forces))\n",
    "mape = 100 * torch.mean(torch.abs((benchmark_forces - forces) / benchmark_forces))\n",
    "\n",
    "print(f\"Mean absolute error: {mae.item()}\")\n",
    "print(f\"Mean absolute percent error: {mape}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1176e-02,  1.1250e-02,  1.2496e-02,  8.0223e-06, -1.8117e-06,\n",
       "         -2.5154e-06]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = segment_ops.aggregate_nodes(\n",
    "    aggregated_features,\n",
    "    n_node,\n",
    "    reduction=\"mean\",\n",
    ")\n",
    "stress = orbff.stress_head.mlp(input)\n",
    "stress = stress.squeeze(-1)\n",
    "stress = orbff.stress_head.output_activation(stress)\n",
    "stress = orbff.stress_head.normalizer.inverse(stress)\n",
    "stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 4.527105193119496e-05\n",
      "Mean absolute percent error: 0.839568018913269%\n"
     ]
    }
   ],
   "source": [
    "mae = torch.mean(torch.abs(benchmark_stress - stress))\n",
    "mape = 100 * torch.mean(torch.abs((benchmark_stress - stress) / benchmark_stress))\n",
    "\n",
    "print(f\"Mean absolute error: {mae.item()}\")\n",
    "print(f\"Mean absolute percent error: {mape}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orb-partitioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
