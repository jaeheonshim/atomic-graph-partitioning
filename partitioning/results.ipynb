{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ase import Atoms\n",
    "from ase.visualize import view\n",
    "from ase.build import make_supercell\n",
    "from ase.io import read\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from mp_api.client import MPRester\n",
    "from ase.io import write\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY=os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "def save_material_ids(output_file=\"material_ids.txt\", max_results=100000, page_size=1000):\n",
    "    with MPRester(API_KEY) as mpr, open(output_file, \"w\") as f:\n",
    "        total_fetched = 0\n",
    "        page = 0\n",
    "\n",
    "        while total_fetched < max_results:\n",
    "            results = mpr.materials.search(all_fields=False)\n",
    "\n",
    "            if not results:\n",
    "                break\n",
    "\n",
    "            for entry in results:\n",
    "                f.write(f\"{entry.material_id},{entry.formula_pretty}\\n\")\n",
    "\n",
    "            total_fetched += len(results)\n",
    "            page += 1\n",
    "\n",
    "        print(f\"Saved {total_fetched} materials to {output_file}\")\n",
    "\n",
    "def get_atoms(id):\n",
    "    with MPRester(API_KEY) as mpr:\n",
    "        structure = mpr.get_structure_by_material_id(id)\n",
    "        \n",
    "        atoms = AseAtomsAdaptor.get_atoms(structure)\n",
    "        \n",
    "        return atoms\n",
    "    \n",
    "# save_material_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper for converting atoms into respective model format (orb or mattersim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_models.forcefield.atomic_system import ase_atoms_to_atom_graphs\n",
    "from mattersim.datasets.utils.convertor import GraphConvertor\n",
    "\n",
    "mattersim_converter = GraphConvertor(\"m3gnet\", 5.0, True, 4.0)\n",
    "\n",
    "def atoms_to_graph_orb(atoms):\n",
    "    graph = ase_atoms_to_atom_graphs(atoms)\n",
    "    senders = graph.senders\n",
    "    receivers = graph.receivers\n",
    "    edge_feats = graph.edge_features\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(graph.n_node))\n",
    "    G.graph['edge_weight_attr'] = 'weight'\n",
    "    \n",
    "    min_dist = min(edge_feats['r'])\n",
    "\n",
    "    for i, u in enumerate(senders):\n",
    "        G.add_edge(u.item(), receivers[i].item(), weight=int(edge_feats['r'][i] / min_dist * 1000))\n",
    "\n",
    "    return G \n",
    "\n",
    "def atoms_to_graph_mattersim(atoms):\n",
    "    return to_networkx(mattersim_converter.convert(atoms, None, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for retrieving relevant metrics from partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_node_count(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    Returns the total number of nodes including overlapping nodes in extended\n",
    "    partitions\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(len(x) for x in extended_partitions)\n",
    "\n",
    "def root_node_count(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    Returns the total number of root nodes (the number of vertices in the\n",
    "    original graph)\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(len(x) for x in partitions)\n",
    "\n",
    "def extended_ratio(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    Returns the ratio of extended nodes to the total number of nodes (higher \n",
    "    ratio means more neighbors were captured in the partition, which we don't\n",
    "    want)\n",
    "    \n",
    "    i.e. how many of our nodes are 'redundant'?\n",
    "    \"\"\"\n",
    "    \n",
    "    root = root_node_count(G, partitions, extended_partitions)\n",
    "    total = total_node_count(G, partitions, extended_partitions)\n",
    "    \n",
    "    return (total - root) / (total)\n",
    "\n",
    "def core_partition_stats(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    Returns statistics about the extended partitions (max, min, mean, std, \n",
    "    range)\n",
    "    \"\"\"\n",
    "    \n",
    "    partition_sizes = [len(p) for p in partitions]\n",
    "    sizes = np.array(partition_sizes)\n",
    "    \n",
    "    return sizes.max(), sizes.min(), sizes.mean(), sizes.std(), (sizes.max() - sizes.mean())\n",
    "\n",
    "def extended_partition_stats(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    Returns statistics about the extended partitions (max, min, mean, std, \n",
    "    range)\n",
    "    \"\"\"\n",
    "    \n",
    "    partition_sizes = [len(p) for p in extended_partitions]\n",
    "    sizes = np.array(partition_sizes)\n",
    "    \n",
    "    return sizes.max(), sizes.min(), sizes.mean(), sizes.std(), (sizes.max() - sizes.mean())\n",
    "\n",
    "def num_cut_edges(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    Returns the number of edges that were cut between partitions\n",
    "    \"\"\"\n",
    "    \n",
    "    node_to_part = {}\n",
    "    for i, part in enumerate(partitions):\n",
    "        for node in part:\n",
    "            node_to_part[node] = i\n",
    "\n",
    "    cut_edges = set()\n",
    "\n",
    "    # Check each edge to see if it crosses partitions\n",
    "    for u, v in G.edges():\n",
    "        pu = node_to_part.get(u)\n",
    "        pv = node_to_part.get(v)\n",
    "        if pu is not None and pv is not None and pu != pv:\n",
    "            cut_edges.add(tuple(sorted((u, v))))\n",
    "\n",
    "    return len(cut_edges)\n",
    "\n",
    "def sum_cut_edge_weights(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    For all cut edges, return sum(x) where x is the weight of each edge\n",
    "    \"\"\"\n",
    "    \n",
    "    cut_edges = set()\n",
    "    node_to_partition = {node: i for i, part in enumerate(partitions) for node in part}\n",
    "    for u, v in G.edges():\n",
    "        if node_to_partition.get(u) != node_to_partition.get(v):\n",
    "            cut_edges.add(tuple(sorted((u, v))))\n",
    "    return sum(G.edges[u, v].get('weight', 0.0) for u, v in cut_edges)\n",
    "\n",
    "def sum_inverse_cut_edge_weights(G, partitions, extended_partitions):\n",
    "    \"\"\"\n",
    "    For all cut edges, return sum(1/x) where x is the weight of each edge\n",
    "    \n",
    "    Maybe minimizing this quantity will help because that means we are\n",
    "    cutting fewer edges of atoms that are closer together (and thus\n",
    "    more likely to have a greater impact on each other)\n",
    "    \"\"\"\n",
    "    \n",
    "    cut_edges = set()\n",
    "\n",
    "    node_to_partition = {}\n",
    "    for i, part in enumerate(partitions):\n",
    "        for node in part:\n",
    "            node_to_partition[node] = i\n",
    "\n",
    "    for u, v in G.edges():\n",
    "        p_u = node_to_partition.get(u)\n",
    "        p_v = node_to_partition.get(v)\n",
    "        if p_u is not None and p_v is not None and p_u != p_v:\n",
    "            cut_edges.add(tuple(sorted((u, v))))\n",
    "\n",
    "    # Sum inverse of edge weights for cut edges\n",
    "    total_inverse_weight = 0.0\n",
    "    for u, v in cut_edges:\n",
    "        w = G.edges[u, v].get('weight', None)\n",
    "        if w is not None and w > 0:\n",
    "            total_inverse_weight += 1.0 / (w)\n",
    "\n",
    "    return total_inverse_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper for running the benchmark and recording metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from partitioning import part_spectral, part_metis, part_grid, part_metis_unweighted\n",
    "\n",
    "def benchmark_trial_orb(atoms, distance=1, granularity=2):\n",
    "    G = atoms_to_graph_orb(atoms)\n",
    "    \n",
    "    print(len(G.edges))\n",
    "\n",
    "    extended_partition_results = []\n",
    "    results = []\n",
    "\n",
    "    for method_name, partition_fn in [\n",
    "        (\"metis\", part_metis),\n",
    "        (\"spectral\", part_spectral),\n",
    "        (\"grid\", part_grid),\n",
    "        (\"metis_unweighted\", part_metis_unweighted)\n",
    "    ]:\n",
    "        if method_name == 'spectral' and len(atoms) >= 10000:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            partitions, extended_partitions = partition_fn(atoms, G, granularity ** 3, distance=distance)\n",
    "\n",
    "            # Core stats\n",
    "            core_max, core_min, core_mean, core_std, core_range = core_partition_stats(G, partitions, extended_partitions)\n",
    "            # Extended stats\n",
    "            ext_max, ext_min, ext_mean, ext_std, ext_range = extended_partition_stats(G, partitions, extended_partitions)\n",
    "\n",
    "            result = {\n",
    "                'method': method_name,\n",
    "                'node_count': total_node_count(G, partitions, extended_partitions),\n",
    "                'root_node_count': root_node_count(G, partitions, extended_partitions),\n",
    "                'extended_ratio': extended_ratio(G, partitions, extended_partitions),\n",
    "                'cut_edges': num_cut_edges(G, partitions, extended_partitions),\n",
    "                'cut_weight_sum': sum_cut_edge_weights(G, partitions, extended_partitions),\n",
    "                'inverse_cut_weight_sum': sum_inverse_cut_edge_weights(G, partitions, extended_partitions),\n",
    "\n",
    "                # Core stats\n",
    "                'core_max': core_max,\n",
    "                'core_min': core_min,\n",
    "                'core_mean': core_mean,\n",
    "                'core_std': core_std,\n",
    "                'core_range': core_range,\n",
    "\n",
    "                # Extended stats\n",
    "                'ext_max': ext_max,\n",
    "                'ext_min': ext_min,\n",
    "                'ext_mean': ext_mean,\n",
    "                'ext_std': ext_std,\n",
    "                'ext_range': ext_range,\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            for part, ext_part in zip(partitions, extended_partitions):\n",
    "                extended_partition_results.append({\n",
    "                    'method': method_name,\n",
    "                    'core_n': len(part),\n",
    "                    'n': len(ext_part)\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{method_name}] Partitioning failed: {e}\")\n",
    "\n",
    "    return results, extended_partition_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load material ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "NUM_MATERIALS = 10\n",
    "MATERIAL_IDS_FILE = 'material_ids.txt'\n",
    "\n",
    "material_ids = []\n",
    "\n",
    "with open(MATERIAL_IDS_FILE, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        material_ids.append(row[0])\n",
    "        \n",
    "material_ids = np.array(material_ids)\n",
    "\n",
    "# choose a random subset\n",
    "material_ids = np.random.choice(material_ids, size=NUM_MATERIALS)\n",
    "material_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import write\n",
    "\n",
    "FULL_DF_FILE = 'results/chunk_0_n_200000_full_mattersim.csv'\n",
    "PARTITION_DF_FILE = 'results/chunk_0_n_200000_partitions_mattersim.csv'\n",
    "\n",
    "all_df = pd.read_csv(FULL_DF_FILE)\n",
    "partition_df = pd.read_csv(PARTITION_DF_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = all_df.groupby(\"method\")[[\n",
    "    \"core_max\", \"core_min\", \"core_mean\",\n",
    "    \"core_std\", \"core_range\"\n",
    "]].mean().round(3)\n",
    "\n",
    "print(summary.to_string(line_width=1000))\n",
    "print('\\n\\n')\n",
    "\n",
    "summary = all_df.groupby(\"method\")[[\n",
    "    \"ext_max\", \"ext_min\", \"ext_mean\",\n",
    "    \"ext_std\", \"ext_range\"\n",
    "]].mean().round(3)\n",
    "\n",
    "print(summary.to_string(line_width=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = partition_df['method'].unique()\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:purple']  # Add more if needed\n",
    "bins = 10\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for method, color in zip(methods, colors):\n",
    "    subset = partition_df[partition_df['method'] == method]['core_n']\n",
    "    plt.hist(subset, bins=bins, alpha=0.6, label=method, color=color)\n",
    "\n",
    "plt.title(\"Core Partition Size by Partitioning Method\")\n",
    "plt.xlabel(\"Core Partition Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(title=\"Method\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = partition_df['method'].unique()\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:purple']  # Add more if needed\n",
    "bins = 10\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for method, color in zip(methods, colors):\n",
    "    subset = partition_df[partition_df['method'] == method]['n']\n",
    "    plt.hist(subset, bins=bins, alpha=0.6, label=method, color=color)\n",
    "\n",
    "plt.title(\"Extended Partition Size by Partitioning Method\")\n",
    "plt.xlabel(\"Extended Partition Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(title=\"Method\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = all_df['method'].unique()\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:purple']  # Add more if needed\n",
    "bins = 10\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for method, color in zip(methods, colors):\n",
    "    subset = all_df[all_df['method'] == method]['extended_ratio']\n",
    "    plt.hist(subset, bins=bins, alpha=0.6, label=method, color=color)\n",
    "\n",
    "plt.title(\"Ratio of Extended Partition Size to All Atoms by Partitioning Method\")\n",
    "plt.xlabel(\"Extended Ratio\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(title=\"Method\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_partitioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
